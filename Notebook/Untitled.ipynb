{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae6815-6c69-42de-93df-9122af44bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Models.misc as misc\n",
    "import Inference.Kolmogorov.performance as performance\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Emu_file_path = '/scratch/ql2221/thermalizer_data/wandb_data/wandb/run-20250526_223850-r12kgbg1/files/checkpoint_best.p'\n",
    "CT_file_path = '/scratch/ql2221/thermalizer_data/wandb_data/wandb/run-20250715_210115-naiteaau/files/checkpoint_last.p'\n",
    "\n",
    "CT = misc.load_diffusion_model(CT_file_path).to(device)\n",
    "Emu = misc.load_model(Emu_file_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9642465a-2e28-4d27-a1d4-39bc2bff30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(\"/scratch/ql2221/thermalizer_data/kolmogorov/reynold10k/test_long.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0cc2bd-8ed5-4c56-8856-bc67d2b17bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 70000, 64, 64])\n",
      "torch.Size([1, 70000, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "data = data_dict[\"data\"]\n",
    "print(data.shape)\n",
    "x = data[0,:,:,:]\n",
    "x = x.unsqueeze(0).to(device)\n",
    "print(x.shape)\n",
    "x = x/4.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b55eb9-2c4b-4d5d-8928-f72ab798aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.6913,  -5.1007,  -4.6001,  -4.6313,  -3.7631,  -4.1202,  -4.8398,\n",
       "          -5.0625,  -5.1515,  -5.0992,  -5.1920,  -5.1410,  -5.0732,  -4.5911,\n",
       "          -5.0131,  -4.7349,  -4.8674,  -4.5680,  -4.6539,  -4.1613,  -3.9007,\n",
       "          -3.9530,  -3.6546,  -3.4271,  -3.8797,  -3.9165,  -3.8641,  -3.9769,\n",
       "          -4.3395,  -3.8093,  -4.2888,  -4.0805,  -4.3550,  -3.9646,  -4.5163,\n",
       "          -4.1582,  -3.7178,  -3.5578,  -3.2317,  -2.7861,  -2.9309,  -2.5412,\n",
       "          -2.0137,  -1.8975,  -2.2074,  -1.7683,  -1.5215,  -1.3681,  -1.5880,\n",
       "          -1.4596,  -1.6486,  -1.7498,  -1.6523,  -1.9130,  -1.7468,  -1.7711,\n",
       "          -1.9896,  -1.9999,  -2.2537,  -2.5596,  -2.4049,  -2.3993,  -2.5670,\n",
       "          -3.0325,  -3.2430,  -2.6593,  -2.4782,  -2.9740,  -2.9751,  -3.3066,\n",
       "          -3.2090,  -2.8442,  -3.1264,  -2.7329,  -2.7533,  -2.4101,  -2.8307,\n",
       "          -2.3625,  -2.6928,  -2.0333,  -2.4297,  -2.3730,  -2.4574,  -2.3911,\n",
       "          -2.1845,  -2.7329,  -2.5440,  -2.3051,  -2.5345,  -2.2370,  -2.4388,\n",
       "          -2.6510,  -2.8503,  -2.9806,  -3.4305,  -3.3781,  -3.5099,  -4.0527,\n",
       "          -4.2176,  -4.0326,  -4.7437,  -4.5258,  -4.7807,  -5.0581,  -4.4658,\n",
       "          -5.0072,  -4.5332,  -5.1707,  -4.6786,  -5.2532,  -4.7728,  -4.8839,\n",
       "          -4.8705,  -5.2391,  -4.7177,  -4.9732,  -5.3853,  -5.4144,  -5.7060,\n",
       "          -5.8264,  -6.2680,  -6.1365,  -6.4050,  -7.2780,  -6.7920,  -6.2839,\n",
       "          -6.8436,  -6.4812,  -6.1216,  -6.1276,  -6.4816,  -6.7398,  -7.1940,\n",
       "          -8.1973,  -8.0582,  -8.5460,  -8.7809,  -9.0681,  -9.8354,  -9.7052,\n",
       "         -10.5091, -10.5424, -11.1728, -11.6219, -11.6148, -12.6325, -11.9109,\n",
       "         -13.5548, -12.6665, -12.7696, -12.8147, -12.5371, -12.7414, -12.7399,\n",
       "         -12.2180, -12.4945, -13.3518, -14.0882, -14.1929, -15.3793, -16.6482,\n",
       "         -16.8160, -16.8781, -17.8006, -16.9313, -17.6044, -17.9420, -17.0333,\n",
       "         -17.3258, -16.3008, -17.7315, -17.2617, -18.4782, -18.8299, -19.7770,\n",
       "         -18.9979, -19.8677, -21.3665, -21.7912, -22.4945, -25.0122, -22.8141,\n",
       "         -25.1077, -26.0180, -26.6670, -27.1888, -27.7979, -27.8312, -27.6599,\n",
       "         -28.2210, -28.6262, -28.0929, -27.9834, -27.5147, -24.6867, -24.9278,\n",
       "         -23.8133, -22.9063, -22.4941, -21.8275, -21.3683, -20.5156, -19.9016,\n",
       "         -18.8743, -19.1438, -18.1033, -18.5481, -17.1800, -16.3037, -16.2451,\n",
       "         -15.3988, -15.3082, -15.5946, -14.9915, -15.1968, -14.6457, -15.1989,\n",
       "         -15.2030, -14.6235, -14.2746, -14.3969, -14.0536, -14.0122, -14.1000,\n",
       "         -13.5961, -13.2245, -13.2721, -13.2538, -13.4403, -13.5070, -12.7971,\n",
       "         -12.9898, -12.4031, -12.2489, -11.9629, -11.6067, -11.6990, -11.1769,\n",
       "         -10.3331,  -9.9852,  -9.0626,  -9.0341,  -8.2585,  -7.4982,  -7.8695,\n",
       "          -7.6111,  -7.1770,  -7.5749,  -7.3054,  -7.0889,  -6.7015,  -6.4304,\n",
       "          -6.6817,  -6.6953,  -6.2776,  -6.0109,  -6.2560,  -5.8023,  -5.5916,\n",
       "          -6.0351,  -5.9645,  -6.0347,  -6.2572,  -5.9642,  -6.0170,  -5.7710,\n",
       "          -5.8776,  -5.7795,  -5.4274,  -5.1883,  -5.2760,  -4.7984,  -4.9150,\n",
       "          -4.9843,  -4.5762,  -4.5284,  -4.1223,  -4.3594,  -4.5813,  -4.0381,\n",
       "          -4.1671,  -4.2444,  -4.1281,  -4.2290,  -4.4305,  -4.2720,  -4.3730,\n",
       "          -4.3001,  -4.2004,  -3.8399,  -4.1370,  -4.4419,  -4.0659,  -3.8819,\n",
       "          -4.1577,  -3.9238,  -3.6363,  -4.4187,  -4.2670,  -4.0280,  -4.1470,\n",
       "          -4.9491,  -4.5638,  -4.5954,  -4.6183,  -4.9466,  -4.6734,  -5.2403,\n",
       "          -5.3713,  -5.4215,  -4.7563,  -5.0310,  -5.1754,  -5.4580,  -4.9281,\n",
       "          -5.5252,  -5.1564,  -5.3164,  -5.0753,  -5.1337,  -5.0541,  -5.0388,\n",
       "          -4.8692,  -4.4060,  -4.5657,  -4.6171,  -4.7218,  -4.6335,  -4.7652,\n",
       "          -4.8879,  -5.1211,  -5.4445,  -5.5419,  -5.9857,  -6.2358,  -6.0962,\n",
       "          -6.5746,  -6.8069,  -6.6822,  -6.7555,  -6.9164,  -7.3285,  -7.8436,\n",
       "          -7.4612,  -7.2878,  -7.0445,  -7.6296,  -7.7754,  -7.5186,  -7.8838,\n",
       "          -7.5665,  -7.7906,  -8.0883,  -7.5299,  -7.8923,  -7.9404,  -7.6323,\n",
       "          -7.8829,  -7.7063,  -7.6003,  -7.8663,  -7.3654,  -7.6547,  -7.1118,\n",
       "          -7.6675,  -6.9870,  -6.7772,  -6.7733,  -6.5149,  -6.3752,  -6.4436,\n",
       "          -6.4812,  -6.0901,  -5.7138,  -5.7453,  -5.9508,  -5.5262,  -5.1233,\n",
       "          -5.6590,  -5.0717,  -5.2028,  -5.1588,  -5.0428,  -4.9649,  -4.9606,\n",
       "          -5.2544,  -4.8274,  -4.4731,  -4.4855,  -4.6840,  -4.7360,  -4.4013,\n",
       "          -4.7469,  -4.8239,  -4.3182,  -4.3581,  -4.3005,  -3.9235,  -4.3659,\n",
       "          -4.5456,  -4.0135,  -4.1733,  -4.1384,  -3.6510,  -4.2440,  -3.4801,\n",
       "          -4.4017,  -4.2005,  -3.8198,  -4.3618,  -4.1071,  -4.1749,  -4.5909,\n",
       "          -4.8625,  -5.5793,  -5.9462,  -5.8465,  -6.0588,  -6.3120,  -6.9012,\n",
       "          -6.6649,  -7.2513,  -7.2552,  -7.9693,  -7.8692,  -7.9360,  -8.7131,\n",
       "          -9.1840,  -8.5164,  -8.8753,  -8.0471,  -8.5823,  -8.8212,  -8.3089,\n",
       "          -8.3738,  -8.6346,  -8.5014,  -8.1205,  -7.8571,  -8.1704,  -8.6169,\n",
       "          -7.4476,  -8.1180,  -7.8733,  -7.3973,  -8.0067,  -7.8916,  -8.3034,\n",
       "          -7.6838,  -8.1119,  -7.8268,  -7.6073,  -7.8920,  -7.9029,  -7.8623,\n",
       "          -8.0305,  -8.0718,  -7.7095,  -7.8543,  -8.1457,  -7.9968,  -7.8205,\n",
       "          -8.4853,  -7.0061,  -7.7375,  -8.3456,  -7.6031,  -8.0624,  -7.8129,\n",
       "          -7.0457,  -7.1940,  -7.2282,  -7.2668,  -6.4078,  -7.1044,  -6.1830,\n",
       "          -6.4371,  -6.5067,  -5.4770,  -5.5514,  -5.8694,  -5.3396,  -4.9003,\n",
       "          -4.7262,  -4.6602,  -4.5146,  -4.3351,  -4.1502,  -4.3776,  -5.0893,\n",
       "          -4.5952,  -5.2834,  -4.7586,  -3.9304,  -4.8000,  -4.7625,  -5.4373,\n",
       "          -5.1450,  -4.3502,  -5.3034,  -5.7796,  -5.5209,  -5.8431,  -6.2167,\n",
       "          -7.4944,  -6.0890,  -6.7145,  -7.1568,  -7.2827,  -6.9618,  -7.5948,\n",
       "          -7.4922,  -7.3363,  -7.5657,  -7.6144,  -7.4244,  -7.1369,  -7.9882,\n",
       "          -7.5214,  -7.1917,  -7.3516,  -6.4332,  -6.6852,  -7.2047,  -5.8362,\n",
       "          -6.2969,  -6.1067,  -5.6802,  -5.9725,  -5.4357,  -5.1275,  -5.2105,\n",
       "          -5.2749,  -4.4578,  -5.6790,  -5.9542,  -5.2187,  -5.8192,  -5.3530,\n",
       "          -5.6642,  -5.4136,  -5.2400,  -4.5654,  -5.4925,  -5.5623,  -5.6192,\n",
       "          -5.8793,  -5.4794,  -5.9594,  -5.4576,  -5.7152,  -5.4769,  -5.5642,\n",
       "          -5.3988,  -5.0500,  -5.3268,  -5.7270,  -5.7806,  -5.2372,  -4.6391,\n",
       "          -5.8885,  -5.7544,  -5.3324,  -5.3303,  -5.0381,  -4.6570,  -5.3517,\n",
       "          -5.4803,  -5.3313,  -4.1572,  -4.7570,  -4.8126,  -4.9720,  -4.3114,\n",
       "          -4.4942,  -4.8437,  -4.2881,  -4.1002,  -4.5117,  -4.4115,  -4.5698,\n",
       "          -4.4192,  -4.3460,  -4.4208,  -3.7717,  -5.0504,  -4.0220,  -4.7967,\n",
       "          -4.5795,  -4.8021,  -4.2085,  -5.1727,  -4.4485,  -4.2208,  -4.6132,\n",
       "          -4.5285,  -4.7718,  -4.3559,  -4.0275,  -5.0330,  -4.8142,  -5.0520,\n",
       "          -5.6174,  -5.1429,  -5.4264,  -5.3924,  -5.9542,  -6.2847,  -4.6941,\n",
       "          -5.3691,  -5.6016,  -5.8418,  -5.2149,  -6.1218,  -5.3160,  -5.1782,\n",
       "          -5.3390,  -5.4052,  -5.2965,  -5.6831,  -4.5570,  -4.9417,  -5.6459,\n",
       "          -4.6820,  -4.9226,  -4.9698,  -5.1143,  -4.5876,  -4.6795,  -3.8751,\n",
       "          -4.3005,  -4.5458,  -4.3587,  -3.8108,  -3.9703,  -3.5910,  -4.1382,\n",
       "          -3.9697,  -3.7160,  -3.4838,  -3.4392,  -3.4943,  -3.3976,  -3.6200,\n",
       "          -3.2228,  -3.3251,  -2.6200,  -3.0911,  -3.6911,  -2.8716,  -3.1246,\n",
       "          -3.3620,  -3.3151,  -3.7267,  -3.2524,  -3.4109,  -3.0429,  -3.4287,\n",
       "          -3.6276,  -3.0347,  -3.3933,  -3.6623,  -4.0253,  -3.5051,  -3.8309,\n",
       "          -3.5313,  -4.3102,  -3.9910,  -3.9427,  -4.3159,  -4.0580,  -4.0749,\n",
       "          -4.0718,  -3.8509,  -4.1194,  -3.9030,  -4.3784,  -4.6049,  -4.6114,\n",
       "          -4.6754,  -4.3031,  -4.7443,  -4.8916,  -4.2929,  -5.1405,  -4.8566,\n",
       "          -5.0079,  -5.0704,  -5.5258,  -5.3349,  -5.5502,  -5.5706,  -6.2184,\n",
       "          -5.7109,  -6.1669,  -6.1109,  -6.3256,  -6.0090,  -6.6813,  -6.2065,\n",
       "          -6.5214,  -6.4797,  -7.5462,  -6.9736,  -7.2533,  -7.6769,  -7.4059,\n",
       "          -8.3504,  -7.8539,  -8.3548,  -8.4686,  -7.4931,  -8.7279,  -8.7344,\n",
       "          -9.3039,  -8.9144,  -9.1239,  -9.2561,  -8.7529,  -9.1037,  -9.5077,\n",
       "          -9.8841,  -9.8308, -10.6790,  -9.4859, -10.8751, -10.7980, -10.1549,\n",
       "         -10.0737,  -9.6020, -11.5836, -11.0037, -10.4374, -12.4660, -11.7897,\n",
       "         -11.1327, -11.5628, -11.3550, -11.5840, -11.9084, -11.9664, -11.9401,\n",
       "         -12.0519, -11.7251, -11.9500, -12.7731, -13.0130, -12.1452, -11.2507,\n",
       "         -12.7256, -12.1455, -12.6790, -12.9624, -13.3003, -12.6175, -11.5174,\n",
       "         -13.4049, -13.2135, -13.7638, -12.7122, -12.6650, -13.1223, -12.4774,\n",
       "         -13.8370, -13.5300, -12.3085, -12.9761, -13.2088, -13.3987, -12.1755,\n",
       "         -13.0212, -12.8112, -12.2246, -12.6736, -12.7208, -11.7897, -12.2734,\n",
       "         -12.1540, -12.0344, -11.4118, -12.1533, -10.5110, -11.8830, -12.1428,\n",
       "         -10.9546, -10.5399, -10.2666, -11.2432, -10.4287, -10.9089, -10.9695,\n",
       "          -9.8749, -10.4492,  -9.5939,  -9.4809,  -9.6270,  -9.1098,  -8.9423,\n",
       "          -9.1387,  -9.0535,  -9.2261,  -8.9568,  -9.0417,  -8.5840,  -8.2065,\n",
       "          -7.8542,  -7.6489,  -7.8631,  -7.8124,  -7.4988,  -7.7088,  -7.6612,\n",
       "          -7.4104,  -7.5123,  -7.2370,  -7.1823,  -7.2112,  -6.9618,  -7.2584,\n",
       "          -7.0271,  -6.8121,  -7.0257,  -6.7155,  -6.5693,  -7.1060,  -6.3428,\n",
       "          -6.8707,  -6.4807,  -6.3426,  -6.3523,  -6.2580,  -6.2003,  -5.8597,\n",
       "          -6.1317,  -5.9363,  -6.3145,  -6.1756,  -6.1416,  -5.6962,  -5.3705,\n",
       "          -5.9313,  -5.4006,  -5.4128,  -5.3759,  -5.4778,  -4.9171,  -5.2117,\n",
       "          -5.1340,  -5.0892,  -4.9308,  -4.9546,  -5.0151,  -4.9194,  -4.6877,\n",
       "          -4.6618,  -4.4827,  -4.5971,  -4.6147,  -4.6537,  -4.3496,  -4.4911,\n",
       "          -4.4064,  -4.2671,  -4.2991,  -4.4257,  -3.9867,  -4.1378,  -4.0960,\n",
       "          -3.9209,  -4.0858,  -3.8648,  -4.1694,  -3.9994,  -4.0204,  -4.2205,\n",
       "          -4.0574,  -4.2743,  -4.0390,  -4.0831,  -4.0402,  -3.9346,  -3.6697,\n",
       "          -3.7746,  -3.5797,  -3.8614,  -3.8655,  -3.8698,  -3.9742,  -3.8585,\n",
       "          -3.7821,  -3.8624,  -3.9546,  -3.6870,  -3.9102,  -3.9083,  -3.8237,\n",
       "          -3.8355,  -3.7045,  -3.8240,  -3.9552,  -3.7618,  -3.7272,  -3.6588,\n",
       "          -3.7694,  -3.9084,  -3.5119,  -3.6594,  -3.7603,  -3.6961,  -3.5428,\n",
       "          -3.8817,  -4.0330,  -3.6059,  -3.7486,  -3.7002,  -3.7849,  -3.7403,\n",
       "          -3.5255,  -3.5094,  -3.4993,  -3.6398,  -3.7750,  -3.5593,  -3.5433,\n",
       "          -3.4489,  -3.6714,  -3.3803,  -3.4797,  -3.6299,  -3.3379,  -3.5701,\n",
       "          -3.5295,  -3.5853,  -3.3079,  -3.3624,  -3.4997,  -3.3087,  -3.4448,\n",
       "          -3.2072,  -3.6070,  -3.4633,  -3.3369,  -3.1913,  -3.5500,  -3.3492,\n",
       "          -3.4322,  -3.3322,  -3.2518,  -3.2423,  -3.2368,  -3.1405,  -3.3301,\n",
       "          -3.2142,  -3.1808,  -3.2433,  -3.1168,  -3.3303,  -3.3331,  -3.1321,\n",
       "          -3.0625,  -3.2407,  -3.1361,  -3.3019,  -3.1739,  -2.9986,  -2.9523,\n",
       "          -2.9344,  -3.0986,  -2.9256,  -2.9566,  -2.9148,  -3.0715,  -3.1337,\n",
       "          -2.9460,  -2.9821,  -2.8172,  -2.9829,  -2.8804,  -2.9392,  -2.9705,\n",
       "          -3.0857,  -2.8850,  -2.9166,  -2.9466,  -2.6851,  -2.6870,  -2.6928,\n",
       "          -2.7634,  -2.8154,  -2.6719,  -2.8146,  -2.7226,  -2.7217,  -2.5516,\n",
       "          -2.8338,  -2.8784,  -2.7108,  -2.7640,  -2.5938,  -6.8723]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t_noised = x[:,1:2]\n",
    "x_t_minus = x[:,0:1]\n",
    "\n",
    "noised_plus_conditional= torch.cat((x_t_noised, x_t_minus), dim=1)\n",
    "delta = torch.tensor([1]).to(device)\n",
    "pred_noise, pred_noise_level = CT.model(noised_plus_conditional,delta,True)\n",
    "pred_noise_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e132cd6d-5353-41a1-a60e-fb8708e957a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m delta = torch.tensor([\u001b[32m1\u001b[39m]).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m rollout, _ = \u001b[43mperformance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_conditional_emu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoising_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRegression\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/Inference/Kolmogorov/performance.py:49\u001b[39m, in \u001b[36mrun_conditional_emu\u001b[39m\u001b[34m(ics, emu, therm, n_steps, lag, denoising_steps, freq, silent, sigma, Regression)\u001b[39m\n\u001b[32m     47\u001b[39m x_t_minus = state_vector[:,aa - freq * lag[\u001b[32m0\u001b[39m].item()].unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     48\u001b[39m noised_plus_conditional = torch.cat((x_t_noised, x_t_minus), dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m pred_noise, pred_noise_level = \u001b[43mtherm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised_plus_conditional\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlag\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m pred_noise = pred_noise.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m pred_noise_level = pred_noise_level.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/Models/Unet.py:440\u001b[39m, in \u001b[36mModernUnetRegressor.forward\u001b[39m\u001b[34m(self, x, delta, regression_output)\u001b[39m\n\u001b[32m    438\u001b[39m x = \u001b[38;5;28mself\u001b[39m.image_proj(x)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lag_embedding:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     delta = \u001b[43mmisc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_timestep_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlag_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     delta = \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28mself\u001b[39m.lag_mlp1(delta))\n\u001b[32m    442\u001b[39m     delta = \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28mself\u001b[39m.lag_mlp2(delta))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/Models/misc.py:46\u001b[39m, in \u001b[36mget_timestep_embedding\u001b[39m\u001b[34m(timesteps, embedding_dim)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_timestep_embedding\u001b[39m(timesteps, embedding_dim: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     43\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    Retrieved from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py#LL90C1-L109C13\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtimesteps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m) == \u001b[32m1\u001b[39m\n\u001b[32m     48\u001b[39m     half_dim = embedding_dim // \u001b[32m2\u001b[39m\n\u001b[32m     49\u001b[39m     emb = math.log(\u001b[32m10000\u001b[39m) / (half_dim - \u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "delta = torch.tensor([1]).to(device)\n",
    "rollout, _ = performance.run_conditional_emu(x[:,0:1], Emu, CT, n_steps=1000, lag = delta, denoising_steps=5, freq = 25, silent=True, sigma=None, Regression = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c815d2-e346-40ab-a7ff-cfb313f0b087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_sing",
   "language": "python",
   "name": "torch_sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
